# List of Papers

- [You canâ€™t solve AI security problems with more AI](https://simonwillison.net/2022/Sep/17/prompt-injection-more-ai/)
- [Adversaries Can Misuse Combinations of Safe Models](https://arxiv.org/abs/2406.14595?utm_source=twitter&utm_campaign=elie)
- [On the Necessity of Auditable Algorithmic Definitions for Machine Unlearning](https://www.usenix.org/system/files/sec22fall_thudi.pdf)
- [Data Poisoning Won't Save You From Facial Recognition](https://arxiv.org/abs/2106.14851)
- [On the Impossible Safety of Large AI Models](https://arxiv.org/abs/2209.15259)
- [Beyond Labeling Oracles: What does it mean to steal ML models?](https://arxiv.org/abs/2310.01959)
- [Text Embeddings Reveal (Almost) As Much As Text](https://arxiv.org/abs/2310.06816?ref=upstract.com)
- [Planting Undetectable Backdoors in Machine Learning Models](https://arxiv.org/abs/2204.06974)
- [Motivating the Rules of the Game for Adversarial Example Research](https://arxiv.org/abs/1807.06732)
- [On Evaluating Adversarial Robustness](https://arxiv.org/abs/1902.06705)
- [LLM Censorship: A Machine Learning Challenge or a Computer Security Problem?](https://arxiv.org/abs/2307.10719)
- [Watermarks in the Sand: Impossibility of Strong Watermarking for Generative Models](https://arxiv.org/abs/2311.04378)
- [When Your AIs Deceive You: Challenges of Partial Observability in Reinforcement Learning from Human Feedback](https://arxiv.org/abs/2402.17747)
- [Adversarial Examples Are Not Bugs, They Are Features](https://arxiv.org/abs/1905.02175)
- [On Adaptive Attacks to Adversarial Example Defenses](https://arxiv.org/abs/2002.08347)
- [Fundamental Tradeoffs between Invariance and Sensitivity to Adversarial Perturbations](https://arxiv.org/abs/2002.04599)
- [Data Authenticity, Consent, & Provenance for AI are all broken: what will it take to fix them?](https://arxiv.org/abs/2404.12691)
- [Proof-of-Learning is Currently More Broken Than You Think](https://ieeexplore.ieee.org/abstract/document/10190491)
- [On the (In)feasibility of ML Backdoor Detection as an Hypothesis Testing Problem](https://arxiv.org/abs/2402.16926)
- [Breach By A Thousand Leaks: Unsafe Information Leakage in `Safe' AI Responses](https://arxiv.org/abs/2407.02551)
- [UnUnlearning: Unlearning is not sufficient for content regulation in advanced generative AI](https://arxiv.org/abs/2407.00106)
